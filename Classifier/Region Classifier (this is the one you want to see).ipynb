{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the \"<i>lyricframe</i>\" method that returns a pandas data frame which consists of all the songs from a folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyricframe(region):\n",
    "\n",
    "\tfrom os import listdir\n",
    "\timport pandas as pd\n",
    "\n",
    "\tlyric_df = pd.DataFrame(columns=['name','lyrics'])\n",
    "\n",
    "\n",
    "\tfor lyricfile in listdir(region):\n",
    "\t    lyricfile = (region)+(lyricfile)\n",
    "\t    #print(lyricfile)\n",
    "\t    rawlyrics= pd.read_json(lyricfile)\n",
    "\t    songs = rawlyrics.get('songs')\n",
    "\t    \n",
    "\t    for x in songs:\n",
    "\t        lyric_df = lyric_df.append({\n",
    "\t            'name': x.get('title'),\n",
    "\t            'lyrics': x.get('lyrics')\n",
    "\t        }, ignore_index=True)\n",
    "\t    \n",
    "\t#lyric_df.shape\n",
    "\treturn(lyric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the lyricframe method for the south and eastcoast folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "south = lyricframe('south\\\\')\n",
    "east = lyricframe('eastcoast\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and generate frequency distribution of each region's data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "eastlyrics_punct = [];\n",
    "southlyrics_punct = [];\n",
    "\n",
    "for n in east['lyrics']:\n",
    "    #print(n)\n",
    "    removal_string = punctuation+(\"\"\"’\"\"\") #genius.com uses a special ’ character\n",
    "    transtable = str.maketrans('','',removal_string)\n",
    "    eastlyrics_punct.append(n.translate(transtable))\n",
    "    \n",
    "    \n",
    "    \n",
    "for n in south['lyrics']:\n",
    "    removal_string = punctuation+(\"\"\"’\"\"\") #genius.com uses a special ’ character\n",
    "    transtable = str.maketrans('','',removal_string)\n",
    "    southlyrics_punct.append(n.translate(transtable))\n",
    "    \n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "#the two most common contractions that aren't in stopwords.words,\n",
    "#as well as words used to denote the sections of a song\n",
    "stop_words.extend(['im','ill','verse','hook','chorus','bridge']) \n",
    "stop_words = set(stop_words) #element removal is faster using set than list\n",
    "\n",
    "word_tokens = nltk.word_tokenize(str(eastlyrics_punct))\n",
    "word_tokens = [w.lower() for w in word_tokens]\n",
    "\n",
    "allwords = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "fdeast = FreqDist(allwords)\n",
    "\n",
    "fdeast.plot(20, cumulative = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_tokens = nltk.word_tokenize(str(southlyrics_punct))\n",
    "word_tokens = [w.lower() for w in word_tokens]\n",
    "\n",
    "allwords = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "fdsouth = FreqDist(allwords)\n",
    "\n",
    "fdsouth.plot(20, cumulative = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionaries to map the 1000 most frequent words overall to integer values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eastlist = fdeast.most_common(1000)\n",
    "southlist = fdsouth.most_common(1000)\n",
    "\n",
    "bothdict = {}\n",
    "\n",
    "for i in range(1000):\n",
    "    bothdict[eastlist[i][0]] = i\n",
    "    \n",
    "southdict = {}\n",
    "\n",
    "for i in range(1000):\n",
    "    southdict[southlist[i][0]] = i\n",
    "    \n",
    "delkeys = []\n",
    "\n",
    "for key in bothdict:\n",
    "    try: \n",
    "        print(southdict[key])\n",
    "    except:\n",
    "        delkeys.append(key)\n",
    "        \n",
    "for key in delkeys:\n",
    "    del bothdict[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import keras.backend  as K\n",
    "K.clear_session()\n",
    "\n",
    "nh = len(east)+len(south)\n",
    "nin = len(bothdict);\n",
    "nout = 1;\n",
    "model = Sequential()\n",
    "model.add(Dense(nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "model.summary()\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "opt = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataframe containing all the training lyrics, and the test data (saved to numpy files for convenience):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords_fd = np.load('trainingwords_fd.npy')\n",
    "Xfab = np.load('Xfab.npy')\n",
    "Xluda = np.load('Xluda.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape input vectors for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "mapdict = {}\n",
    "\n",
    "for key in bothdict:\n",
    "    mapdict.update({key:i})\n",
    "    i+=1\n",
    "\n",
    "X = []\n",
    "\n",
    "for i in range(nh):\n",
    "    X.append([allwords_fd[i][key]*mapdict[key] for key in bothdict])\n",
    "    \n",
    "Xtr = np.array(X)\n",
    "y = np.zeros(4336)\n",
    "y[0:1721]=1;\n",
    "y[1722:]=0\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isiteast = model.fit(Xtr, y, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(Xfab)\n",
    "model.predict(Xluda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
